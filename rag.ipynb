{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramayana RAG System ğŸ¹\n",
    "\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG)** system using the ancient Indian epic **Ramayana** as the knowledge base.\n",
    "\n",
    "## How to Use:\n",
    "1. Run all cells in order (Ctrl+Shift+Enter for each cell)\n",
    "2. Once setup is complete, use: `ask_ramayana_question(\"your question here\")`\n",
    "3. Test with sample questions about Rama, Sita, Hanuman, and other characters\n",
    "\n",
    "## Features:\n",
    "- ğŸ” Semantic search through the Ramayana text\n",
    "- ğŸ¤– AI-powered responses using Azure OpenAI\n",
    "- ğŸ“š Comprehensive coverage of the epic\n",
    "- âš¡ Fast vector-based retrieval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceTransformersTokenTextSplitter \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Azure OpenAI models...\n",
      "âœ“ Found working embedding deployment: 'text-embedding-3-large'\n",
      "âœ“ Azure OpenAI models initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI models with error handling\n",
    "print(\"Initializing Azure OpenAI models...\")\n",
    "\n",
    "# Try to find working embedding deployment\n",
    "embedding_deployments_to_try = [\"text-embedding-3-large\", \"text-embedding-ada-002\", \"text-embedding-3-small\"]\n",
    "\n",
    "embedding_model = None\n",
    "for deployment_name in embedding_deployments_to_try:\n",
    "    try:\n",
    "        test_embedding_model = AzureOpenAIEmbeddings(\n",
    "            azure_deployment=deployment_name,\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "        )\n",
    "        # Test if the deployment works\n",
    "        test_embed = test_embedding_model.embed_query(\"test\")\n",
    "        print(f\"âœ“ Found working embedding deployment: '{deployment_name}'\")\n",
    "        embedding_model = test_embedding_model\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— '{deployment_name}' failed: {e}\")\n",
    "        continue\n",
    "\n",
    "if embedding_model is None:\n",
    "    raise Exception(\"No working embedding deployment found. Please check your Azure OpenAI configuration.\")\n",
    "\n",
    "# Initialize chat model  \n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Azure OpenAI models initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found Ramayana text file: c:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\input\\ramayana.txt\n",
      "Loading Ramayana text...\n",
      "âœ“ Document loaded and split into 37 chunks\n",
      "Sample chunk: The Ramayana - Epic of Sri Rama\n",
      "===============================\n",
      "\n",
      "This is a comprehensive collection of the Ramayana, one of the greatest epics of ancient India, composed by the sage Valmiki. The Ramay...\n"
     ]
    }
   ],
   "source": [
    "# Set up file paths\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, \"input\", \"ramayana.txt\")\n",
    "db_dir = os.path.join(current_dir, \"ramayana_db\")\n",
    "\n",
    "# Check if the Ramayana text file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist. Please make sure you have the ramayana.txt file in the input folder.\")\n",
    "\n",
    "print(f\"âœ“ Found Ramayana text file: {file_path}\")\n",
    "\n",
    "# Load and process the document\n",
    "print(\"Loading Ramayana text...\")\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200  # Some overlap to maintain context\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ“ Document loaded and split into {len(docs)} chunks\")\n",
    "print(f\"Sample chunk: {docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up vector store...\n",
      "Loading existing vector store...\n",
      "âœ“ Vector store loaded successfully\n",
      "âœ“ Retriever configured successfully\n"
     ]
    }
   ],
   "source": [
    "# Create or load vector store\n",
    "print(\"Setting up vector store...\")\n",
    "\n",
    "persistent_directory = os.path.join(db_dir, \"ramayana_faiss_db\")\n",
    "\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Creating new vector store...\")\n",
    "    os.makedirs(db_dir, exist_ok=True)\n",
    "    \n",
    "    # Create FAISS vector store with embeddings\n",
    "    vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "    \n",
    "    # Save the vector store\n",
    "    vector_store.save_local(persistent_directory)\n",
    "    print(f\"âœ“ Vector store created and saved to {persistent_directory}\")\n",
    "else:\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = FAISS.load_local(\n",
    "        persistent_directory, \n",
    "        embedding_model, \n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print(\"âœ“ Vector store loaded successfully\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Return top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retriever configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing retrieval with query: 'Who is Hanuman and what are his powers?'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mWho is Hanuman and what are his powers?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTesting retrieval with query: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m relevant_docs = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Retrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(relevant_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m relevant documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(relevant_docs[:\u001b[32m2\u001b[39m]):  \u001b[38;5;66;03m# Show first 2 results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\langchain_core\\retrievers.py:261\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    265\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1078\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1082\u001b[39m     docs_and_similarities = (\n\u001b[32m   1083\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1084\u001b[39m             query, **kwargs_\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m   1086\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:643\u001b[39m, in \u001b[36mFAISS.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    624\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    625\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    629\u001b[39m     **kwargs: Any,\n\u001b[32m    630\u001b[39m ) -> List[Document]:\n\u001b[32m    631\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    632\u001b[39m \n\u001b[32m    633\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    641\u001b[39m \u001b[33;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:516\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m \u001b[33;03m    L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    515\u001b[39m embedding = \u001b[38;5;28mself\u001b[39m._embed_query(query)\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:417\u001b[39m, in \u001b[36mFAISS.similarity_search_with_score_by_vector\u001b[39m\u001b[34m(self, embedding, k, filter, fetch_k, **kwargs)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._normalize_L2:\n\u001b[32m    416\u001b[39m     faiss.normalize_L2(vector)\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m scores, indices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m docs = []\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\285104\\Downloads\\Langchain-RAG\\Langchain-RAG\\venv\\Lib\\site-packages\\faiss\\class_wrappers.py:349\u001b[39m, in \u001b[36mhandle_Index.<locals>.replacement_search\u001b[39m\u001b[34m(self, x, k, params, D, I, numeric_type)\u001b[39m\n\u001b[32m    347\u001b[39m n, d = x.shape\n\u001b[32m    348\u001b[39m x = np.ascontiguousarray(x, _numeric_to_str(numeric_type))\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m d == \u001b[38;5;28mself\u001b[39m.d\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m k > \u001b[32m0\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test retrieval functionality\n",
    "test_query = \"Who is Hanuman and what are his powers?\"\n",
    "print(f\"Testing retrieval with query: '{test_query}'\")\n",
    "\n",
    "relevant_docs = retriever.invoke(test_query)\n",
    "print(f\"âœ“ Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs[:2]):  # Show first 2 results\n",
    "    print(f\"\\nDocument {i+1} excerpt:\")\n",
    "    print(doc.page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG question-answering function ready!\n"
     ]
    }
   ],
   "source": [
    "def ask_ramayana_question(question):\n",
    "    \"\"\"\n",
    "    Ask a question about the Ramayana and get an AI-powered answer\n",
    "    \n",
    "    Args:\n",
    "        question (str): Your question about the Ramayana\n",
    "        \n",
    "    Returns:\n",
    "        str: AI-generated answer based on relevant text chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = retriever.invoke(question)\n",
    "        \n",
    "        # Prepare context from retrieved documents\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"Based on the following excerpts from the Ramayana epic, please answer the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Content from Ramayana:\n",
    "{context}\n",
    "\n",
    "Please provide a detailed and accurate answer based only on the provided content. If the answer cannot be found in the content, say \"I don't have enough information to answer this question based on the provided content.\"\n",
    "\"\"\"\n",
    "        \n",
    "        # Get response from chat model\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a knowledgeable assistant specializing in the Ramayana epic. Provide accurate and detailed answers based on the given content.\"),\n",
    "            HumanMessage(content=prompt)\n",
    "        ]\n",
    "        \n",
    "        response = chat_model.invoke(messages)\n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {str(e)}\"\n",
    "\n",
    "print(\"âœ“ RAG question-answering function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Ramayana RAG System\n",
      "==================================================\n",
      "\n",
      "ğŸ“ Question 1: Who is Hanuman and what are his special powers?\n",
      "--------------------------------------------------\n",
      "ğŸ’¬ Answer: Error occurred: \n",
      "==================================================\n",
      "\n",
      "ğŸ“ Question 2: Why was Rama exiled from Ayodhya?\n",
      "--------------------------------------------------\n",
      "ğŸ’¬ Answer: Error occurred: \n",
      "==================================================\n",
      "\n",
      "ğŸ“ Question 3: How did Rama and Sita first meet?\n",
      "--------------------------------------------------\n",
      "ğŸ’¬ Answer: Error occurred: \n",
      "==================================================\n",
      "\n",
      "ğŸ‰ RAG system testing complete!\n",
      "âœ“ You can now use ask_ramayana_question('your question') to ask about the Ramayana\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with sample questions\n",
    "print(\"ğŸ§ª Testing Ramayana RAG System\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_questions = [\n",
    "    \"Who is Hanuman and what are his special powers?\",\n",
    "    \"Why was Rama exiled from Ayodhya?\",\n",
    "    \"How did Rama and Sita first meet?\",\n",
    "    \"What happened during the battle between Rama and Ravana?\",\n",
    "    \"Who helped Rama build the bridge to Lanka?\",\n",
    "    \"What are the main teachings of the Ramayana?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(sample_questions[:3], 1):  # Test first 3 questions\n",
    "    print(f\"\\nğŸ“ Question {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    answer = ask_ramayana_question(question)\n",
    "    print(f\"ğŸ’¬ Answer: {answer}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ‰ RAG system testing complete!\")\n",
    "print(\"âœ“ You can now use ask_ramayana_question('your question') to ask about the Ramayana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive interface for asking questions\n",
    "print(\"ğŸ¯ Interactive Ramayana Q&A Interface\")\n",
    "print(\"=\"*40)\n",
    "print(\"Your Ramayana RAG system is ready!\")\n",
    "print()\n",
    "print(\"ğŸ“– Sample questions you can ask:\")\n",
    "print(\"â€¢ 'What powers does Hanuman have?'\")\n",
    "print(\"â€¢ 'How did Sita prove her purity?'\")\n",
    "print(\"â€¢ 'Why did Vibhishana join Rama?'\")\n",
    "print(\"â€¢ 'What happened to Ravana in the end?'\")\n",
    "print(\"â€¢ 'How was the bridge to Lanka built?'\")\n",
    "print()\n",
    "print(\"ğŸ’¡ Usage examples:\")\n",
    "print(\"   answer = ask_ramayana_question('Who is the main character of Ramayana?')\")\n",
    "print(\"   print(answer)\")\n",
    "print()\n",
    "print(\"ğŸš€ Ready to answer your questions about the Ramayana!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
